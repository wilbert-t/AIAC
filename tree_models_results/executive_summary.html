
<!DOCTYPE html>
<html>
<head>
    <title>Tree-Based Models Analysis - Executive Summary</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }
        .header { background-color: #f4f4f4; padding: 20px; border-radius: 5px; }
        .section { margin: 30px 0; }
        .metrics { display: flex; flex-wrap: wrap; gap: 20px; }
        .metric-card { background-color: #e8f4fd; padding: 15px; border-radius: 5px; min-width: 200px; }
        table { border-collapse: collapse; width: 100%; }
        th, td { border: 1px solid #ddd; padding: 12px; text-align: left; }
        th { background-color: #f2f2f2; }
        .best-model { background-color: #d4edda; }
        .recommendation { background-color: #fff3cd; padding: 15px; border-radius: 5px; border-left: 4px solid #ffc107; }
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸŒ³ Tree-Based Models Analysis - Executive Summary</h1>
        <p><strong>Generated:</strong> 2025-10-01 12:08:48</p>
        <p><strong>Dataset:</strong> Au Cluster Energy Prediction</p>
    </div>
    
    <div class="section">
        <h2>ğŸ“Š Key Performance Metrics</h2>
        <div class="metrics">
            <div class="metric-card">
                <h3>ğŸ† Best Model</h3>
                <p><strong>Xgboost</strong></p>
                <p>Test RÂ²: 0.9162</p>
            </div>
            <div class="metric-card">
                <h3>ğŸ“ˆ Performance</h3>
                <p>RMSE: 0.8193</p>
                <p>MAE: 0.5567</p>
            </div>
            <div class="metric-card">
                <h3>ğŸ¯ Cross-Validation</h3>
                <p>CV RÂ²: 0.9165 Â± 0.0107</p>
            </div>
            <div class="metric-card">
                <h3>ğŸ“‹ Dataset Info</h3>
                <p>Training: 799 samples</p>
                <p>Test: 200 samples</p>
                <p>Features: 30</p>
            </div>
        </div>
    </div>
    
    <div class="section">
        <h2>ğŸ† Model Performance Ranking</h2>
        <table>
            <tr>
                <th>Rank</th>
                <th>Model</th>
                <th>Test RÂ²</th>
                <th>Test RMSE</th>
                <th>Test MAE</th>
                <th>CV RÂ² (Mean Â± Std)</th>
                <th>Model Type</th>
            </tr>
            <tr class="best-model">
                <td>1</td>
                <td><strong>Xgboost</strong></td>
                <td>0.9162</td>
                <td>0.8193</td>
                <td>0.5567</td>
                <td>0.9165 Â± 0.0107</td>
                <td>Gradient Boosting</td>
            </tr>
            <tr class="">
                <td>2</td>
                <td><strong>Lightgbm</strong></td>
                <td>0.8850</td>
                <td>0.9597</td>
                <td>0.6352</td>
                <td>0.8963 Â± 0.0175</td>
                <td>Gradient Boosting (Fast)</td>
            </tr>
            <tr class="">
                <td>3</td>
                <td><strong>Extra Trees</strong></td>
                <td>0.8811</td>
                <td>0.9758</td>
                <td>0.6634</td>
                <td>0.8692 Â± 0.0220</td>
                <td>Tree-Based</td>
            </tr>
            <tr class="">
                <td>4</td>
                <td><strong>Random Forest</strong></td>
                <td>0.8599</td>
                <td>1.0591</td>
                <td>0.7257</td>
                <td>0.8546 Â± 0.0243</td>
                <td>Ensemble (Bagging)</td>
            </tr>
            <tr class="">
                <td>5</td>
                <td><strong>Knn Stable</strong></td>
                <td>0.8093</td>
                <td>1.2359</td>
                <td>0.8514</td>
                <td>0.8015 Â± 0.0298</td>
                <td>Tree-Based</td>
            </tr>
        </table>
    </div>
    
    <div class="section">
        <h2>ğŸ’¡ Key Insights & Recommendations</h2>
        <div class="recommendation"><p><strong>â€¢</strong> The Xgboost model achieved the highest performance with RÂ² = 0.9162, explaining 91.6% of the variance in Au cluster energies.</p><p><strong>â€¢</strong> Significant performance differences observed (Î”RÂ² = 0.1069), indicating that the choice of tree-based algorithm significantly impacts Au cluster energy prediction.</p><p><strong>â€¢</strong> Feature importance analysis identifies 'soap_pc_2' as the most critical predictor across tree-based models, suggesting its fundamental role in Au cluster stability.</p><p><strong>â€¢</strong> The Xgboost model shows the most stable performance across cross-validation folds, indicating robust generalization capability.</p><p><strong>â€¢</strong> The Xgboost model demonstrates excellent generalization with minimal overfitting (gap: 0.082).</p>
        </div>
    </div>
    
    <div class="section">
        <h2>ğŸ”¬ Technical Analysis</h2><p><strong>Random Forest:</strong> Residuals are slightly biased (mean residual: -0.0427) and non-normally distributed (Shapiro-Wilk p = 0.0000).</p><p><strong>Xgboost:</strong> Residuals are slightly biased (mean residual: -0.0967) and non-normally distributed (Shapiro-Wilk p = 0.0000).</p><p><strong>Lightgbm:</strong> Residuals are slightly biased (mean residual: -0.0332) and non-normally distributed (Shapiro-Wilk p = 0.0000).</p><p><strong>Extra Trees:</strong> Residuals are slightly biased (mean residual: -0.0174) and non-normally distributed (Shapiro-Wilk p = 0.0000).</p><p><strong>Knn Stable:</strong> Residuals are slightly biased (mean residual: 0.1918) and non-normally distributed (Shapiro-Wilk p = 0.0000).</p><p>Feature analysis: 15 structural descriptors and 15 SOAP descriptors were used for tree-based model training.</p><p><strong>Random Forest learning curve:</strong> Model appears slightly overfitted (train-validation gap: 0.1255).</p><p><strong>Xgboost learning curve:</strong> Model appears slightly overfitted (train-validation gap: 0.0817).</p><p><strong>Lightgbm learning curve:</strong> Model appears slightly overfitted (train-validation gap: 0.0997).</p><p><strong>Extra Trees learning curve:</strong> Model appears slightly overfitted (train-validation gap: 0.1304).</p><p><strong>Knn Stable learning curve:</strong> Model appears overfitted (train-validation gap: 0.1985).</p>
    </div>
    
    <div class="section">
        <h2>ğŸ“ Generated Files</h2>
        <ul>
            <li><strong>model_performance_comparison.csv:</strong> Detailed performance metrics</li>
            <li><strong>all_predictions.csv:</strong> Predictions from all models</li>
            <li><strong>feature_importance files:</strong> Feature importance analysis per model</li>
            <li><strong>Individual model folders:</strong> Detailed plots for each model</li>
            <li><strong>Combined analysis plots:</strong> Cross-model comparisons</li>
            <li><strong>Trained models:</strong> Saved as .joblib files</li>
        </ul>
    </div>
    
    <div class="section">
        <h2>ğŸš€ Next Steps</h2>
        <ol>
            <li><strong>Production Deployment:</strong> Use the {summary_stats['best_model']} model for predictions</li>
            <li><strong>Feature Analysis:</strong> Investigate top-performing features for physicochemical insights</li>
            <li><strong>Model Validation:</strong> Test on additional Au cluster configurations</li>
            <li><strong>Hyperparameter Optimization:</strong> Fine-tune the best performing model further</li>
            <li><strong>Ensemble Methods:</strong> Consider combining top 2-3 models for improved predictions</li>
            <li><strong>SHAP Analysis:</strong> Apply SHAP values for detailed feature attribution</li>
        </ol>
    </div>
    
</body>
</html>